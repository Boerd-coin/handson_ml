{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d07f9e0e-0e07-4017-80d6-9a595a4b37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    %pip install -q -U tensorflow-addons\n",
    "    %pip install -q -U transformers\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"nlp\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2933ce-41c6-40f0-aa67-50112f4e3f24",
   "metadata": {},
   "source": [
    "# 字符RNN：Char-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad3e06-6f73-4c9e-83bc-e3ba4e5ab563",
   "metadata": {},
   "source": [
    "## 打乱一个序列的移动窗口，再将乱序的窗口放到不同的很多不同的批次里"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ffae75-b9f2-42d6-a846-61805549e408",
   "metadata": {},
   "source": [
    "例如，一个0-14的序列，窗口长为5，窗口移动步长为2(如,`[0, 1, 2, 3, 4]`, `[2, 3, 4, 5, 6]`, 等等...)，打乱每个窗口的顺序，然后从每个窗口中划分出输入inputs(窗口内前4个步长)和目标targets(窗口内前4个步长) (如窗口, `[2, 3, 4, 5, 6]` 将会被划分成 `[[2, 3, 4, 5], [3, 4, 5, 6]]`)，之后要创建3个这样的input/target对，如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b93cf04-a8fa-4636-8682-8f34c0df2e3e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Batch 0 \n",
      "X_batch\n",
      "[[6 7 8 9]\n",
      " [2 3 4 5]\n",
      " [4 5 6 7]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 7  8  9 10]\n",
      " [ 3  4  5  6]\n",
      " [ 5  6  7  8]]\n",
      "____________________ Batch 1 \n",
      "X_batch\n",
      "[[ 0  1  2  3]\n",
      " [ 8  9 10 11]\n",
      " [10 11 12 13]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 1  2  3  4]\n",
      " [ 9 10 11 12]\n",
      " [11 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n_steps = 5\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
    "# window()方法会创建包含窗口的数据集，数据集中的每个窗口也表示为一个数据集\n",
    "# 也即是说它创建的是一个嵌套数据集，类似于列表的列表\n",
    "# 使用嵌套数据集的好处是：\n",
    "# 通过调用窗口的数据集方法，可以同时对所有窗口进行操作，例如混洗或批处理\n",
    "dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
    "# 但是RNN不接收嵌套的数据集，它接收的是张量。所以接下来需要使用flat_map()方法将其展开\n",
    "# 例如有一个嵌套数据集{{1, 2}, {3, 4, 5, 6}}，可将其展平为{1, 2, 3, 4, 5, 6}\n",
    "# flat_map()方法还可以接收一个函数作为参数，它允许在嵌套数据集展平前变换嵌套数据集中的每个窗口\n",
    "# 比如传入函数lambda ds: ds.batch(2)，这将是嵌套数据集展开为张量长度为2的数据集: {[1, 2], [3, 4], [5, 6]}\n",
    "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
    "dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
    "dataset = dataset.batch(3).prefetch(1)\n",
    "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n",
    "    print(X_batch.numpy())\n",
    "    print(\"=\" * 5, \"\\nY_batch\")\n",
    "    print(Y_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a8c8d4-f982-46fb-a829-5de4a87d4022",
   "metadata": {},
   "source": [
    "## 载入数据 & 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1dbba99-33eb-47da-bde1-6a2f05274224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入莎士比亚的作品集\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defb3b14-3c29-4d07-9ded-166ca5768f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea5b02e-6961-484c-8b85-34b797574b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c2ee2-4658-4d30-85b7-885bedb2525c",
   "metadata": {},
   "source": [
    "我们可以使用第13 章的内容使用预处理层将字符编码，或者可以使用Tokenizer函数来处理字符数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6c607f-0cf4-4a20-9f84-4035de0b3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要设定char_level=True，不然默认按照单词级别进行编码\n",
    "# 在真正对字符进行编码前，需要先fit\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62fed51d-6160-4624-89c6-1cf426458c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dbb45d6-46bf-4105-84aa-3685f930136c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16f92e-e411-4b45-8a6f-8e8a32e12930",
   "metadata": {},
   "source": [
    "在字符编码fit完成之后就可以通过tokenizer对象来查看字符的数量 或 字符总数量等信息了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5f9638-f2d8-49a1-9ca7-2a88a72c23cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不重复字符数量\n",
    "max_id = len(tokenizer.word_index)\n",
    "# 字符总数量\n",
    "dataset_size = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac98d89e-3e8f-4b05-806a-1fdf26f118db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为Tokenizer默认以数字1开始编码，如果想要从0开始编码的话，就手动减1\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d629d1a0-0934-4fbd-b8f3-6c80feaa0d91",
   "metadata": {},
   "source": [
    "**注意**: 在以前，我们需要使用 `dataset.repeat()` 来使数据集 \"无限\"，然后在调用 `model.fit()` 时还需要设置 `steps_per_epoch` 参数。 以前不这么做会出现很多的Tensorflow的bug。 然而现在这些bug都被修复了，可以简化代码了，再也不需要 `dataset.repeat()` 或者 `steps_per_epoch` 了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b5fcd34-c631-42db-8fc9-ee6d2cb0bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN不可能在整个序列上进行循环，所以需要将其分成很多小序列\n",
    "# 数据集中的实例将是一个个短的固定长度的字符串，RNN将会在这些短字符串的长度展开\n",
    "# 这就是时间截断反向传播\n",
    "# n_steps越小，RNN学习就越容易，但是RNN不能学习比n_steps更长的模式，所以n_steps不要太短\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "897de763-d038-4f69-96f1-9045274b18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "# 为什么这里需要使用独热编码？\n",
    "# 1、首先RNN在处理顺序序列时，其实会隐含地假设当前学到的模式在以后会一直重复的出现；\n",
    "# 2、在学习中，若训练集中示例相互独立且分布相同时，梯度下降的效果最好。\n",
    "# 在这里，每行字符串只有39个字符(时间序列中是featrues)，所以可以考虑使用独热编码\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ae70543-312a-4652-9031-2c7b130db9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_venv",
   "language": "python",
   "name": "tenserflow_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
