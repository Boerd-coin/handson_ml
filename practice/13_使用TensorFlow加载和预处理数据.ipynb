{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485a003c-980d-42d5-b6e3-6bd91afbf36a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "if IS_COLAB or IS_KAGGLE:\n",
    "    %pip install -q -U tfx\n",
    "    print(\"You can safely ignore the package incompatibility errors.\")\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"data\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e42b04-4d4d-4d94-85d2-5aad6438149d",
   "metadata": {},
   "source": [
    "## 什么是数据API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "843e4cd2-3f84-4847-85d1-aafdaea195b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先来创建一个小数据集来做示例\n",
    "X = tf.range(10)\n",
    "# from_tensor_slices可以将X的切片（沿第一维度去切）组合成一个tf.data.Dataset对象\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eace5c3a-de31-41d1-a540-ea3bceb1ae4f",
   "metadata": {},
   "source": [
    "就相当于这么做:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3bdeeb6-e611-45af-8378-fb0468b10826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5395c1d1-979d-43e0-95b4-7f3f17a798fc",
   "metadata": {},
   "source": [
    "### 链式转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b315f-4d7a-43d6-8c88-5d2c5711877c",
   "metadata": {},
   "source": [
    "将数据集放入到Dataset对象之后就可以使用链式表达式来对其进行操作了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96471ad1-943c-4eef-9348-221b2a154bcc",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 比方说将原数据集复制三次，然后每批7个元素分批输出\n",
    "# 这里要注意，使用repeat方法复制并不会真的在内存中复制3次，并且如果不给repeat()方法传参，他就会复制无数遍数据集\n",
    "# 虽然不会真的复制，链式转换返回的数据集是真是存在于内存中的\n",
    "# 对于batch方法，向其中传递的参数是每批次中元素的个数，可看到最后的一批中只有两个元素\n",
    "# 如果在batch()方法中设置rop_remainder = True，那么就可以删除不满足元素数量要求的批\n",
    "dataset = dataset.repeat(3).batch(7, drop_remainder = False)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ca1090-4d6b-48d7-8c96-c24877b76753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用map方法可以操作数据集中所有元素\n",
    "# 比如说要将数据集中的元素都乘2\n",
    "dataset = dataset.map(lambda x: x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6901acf7-3e69-469f-ad67-f7e88b841ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f28f841e-742f-47d9-b3b1-bc563ed8190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用unbatch方法可以将所有批次中的元素都变成一个张量\n",
    "dataset = dataset.unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61e70828-1ce5-407b-b7d9-7da3d10f600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用filter方法可以简单对每个元素进行过滤\n",
    "# 下面语句的含义是：值保留小于10的元素\n",
    "dataset = dataset.filter(lambda x: x < 10)  # keep only items < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1029b575-fb85-4e8b-8cfe-539a41a7da3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e12c3-10bf-4643-9cdf-8121ba769ed9",
   "metadata": {},
   "source": [
    "### 乱序数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf83c5f-7cfe-4bb5-8024-b6d96af1e05d",
   "metadata": {},
   "source": [
    "已知，当训练集中各实例都相互独立且分布均匀的时候，梯度下降最快  \n",
    "所以使用数据API的shuffle()方法来对数据集进行混洗就很有必要了  \n",
    "实现混洗的原理是，在一开始把数据集的前几个元素放进缓冲区中，每次从缓冲区中随机抽出一个实例，并从剩余实例中抽一个实例放进缓冲区中，直到数据集中所有的实例都遍历完成  \n",
    "  \n",
    "这就像是洗牌，左边有一堆牌，最开始只拿上面的三张牌，随机抽出一张牌放到右边，然后再从左边拿一张牌，再从三张牌中随机抽出一张牌放到右边，...直到所有的牌都在右边  \n",
    "这里的三张牌指的就是缓冲区的容量，很明显，缓冲区得大一点效果才能好。但是实践证明并不能太大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "175a34f5-a8b6-49c3-b4ed-935d0cda2cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 3 0 4 2 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 7 1 0 3 2 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 6 9 8 9 7 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 1 4 5 2 8 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([6 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# 使用range方法创建一个Dataset对象，重复3次\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "# 将数据集混洗，缓冲区容量为3，每个批次元素数量为7\n",
    "dataset = dataset.shuffle(buffer_size=3, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c2b63f8-c83d-48d8-bdc0-98f88a5aadef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 3 0 2 6 7 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9 4 0 2 1 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 6 7 8 9 1 2], shape=(7,), dtype=int64)\n",
      "tf.Tensor([0 5 4 6 3 5 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8], shape=(2,), dtype=int64)\n",
      "tf.Tensor([0 1 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([2 8 1 2 0 9 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([6 5 7 4 0 9 2], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 3 5 8 4 7 8], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 6], shape=(2,), dtype=int64)\n",
      "tf.Tensor([2 3 1 0 5 6 8], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 9 0 4 3 4 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 8 7 6 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([2 1 4 5 3 7 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 8], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 如果在原本已经混洗过的数据集上再复制一次，也是一个非常好的点子\n",
    "# 并且此时复制出的不仅仅是元素，还有混洗的操作\n",
    "# 可以看到复制出来的批也是混洗之后的\n",
    "for item in dataset.repeat(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "657387d6-4b3d-4fa1-9f5e-384f37cd35b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 5 6 4 8], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 0 1 1 7 3 2], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 7 8 9 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 4 6 7 3 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 5], shape=(2,), dtype=int64)\n",
      "tf.Tensor([0 2 3 5 6 4 8], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 0 1 1 7 3 2], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 7 8 9 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 4 6 7 3 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 5], shape=(2,), dtype=int64)\n",
      "tf.Tensor([0 2 3 5 6 4 8], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 0 1 1 7 3 2], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 7 8 9 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 4 6 7 3 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 5], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 如果想在复制的同时不打乱原有混洗的顺序，可以在shuffle()方法中设置reshuffle_each_iteration = False\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=3, seed=42, reshuffle_each_iteration = False).batch(7)\n",
    "\n",
    "# 此时再对混洗之后的批数据集进行复制，就会保持原有的混洗顺序进行复制了\n",
    "for item in dataset.repeat(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6acf9-9385-4526-8a5a-d357f68100ec",
   "metadata": {},
   "source": [
    "### 同时对多个CSV文件进行混洗"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4433d21-1d0c-433e-93db-ff482e1e7851",
   "metadata": {},
   "source": [
    "当面临的数据集非常非常大的时候，使用shuffle对数据集进行混洗可能就不太够用了，毕竟缓冲区相对于整个数据集的大小来说可能有点小马拉大车了  \n",
    "但是可以在使用shuffle之前使用别的方法来对原始完整数据集进行一个预混洗，然后再使用shuffle  \n",
    "  \n",
    "  \n",
    "或者将原数据集分成很多个子文件，然后使用shuffle分别对各个子文件进行混洗后再组合到一起，但是这么做有可能会导致同一个文件中的数据仍然十分接近。  \n",
    "为了避免这种情况可以同时随机读取多个文件的数据，并且使用shuffle方法建立一个公用的缓冲区。虽然这听起来很麻烦，其中还得会用到多线程，Data API只需要几行代码就能实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "081f9437-fb03-4db0-9b45-e13d5f4dedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这次拿加利福尼亚的housing数据来做示例\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "\n",
    "# 准备好训练集的数据，等会儿就混洗训练集的数据\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc3b77-f47b-4819-9145-5c9952756f26",
   "metadata": {},
   "source": [
    "#### 将数据集分成多个csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca5182c5-d582-4eca-aea9-7245adb735c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "    # 在文件都保存好之后返回所有文件的地址\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25e81fed-2f32-4608-b7a8-0169829b9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f615fd3-d1a6-4fa3-b181-19a4babd0293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5214</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.049945</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>1.605993</td>\n",
       "      <td>37.63</td>\n",
       "      <td>-122.43</td>\n",
       "      <td>1.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.3275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.490060</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>3.443340</td>\n",
       "      <td>33.69</td>\n",
       "      <td>-117.39</td>\n",
       "      <td>1.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.542373</td>\n",
       "      <td>1.591525</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>2.250847</td>\n",
       "      <td>38.44</td>\n",
       "      <td>-122.98</td>\n",
       "      <td>1.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.1736</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.289003</td>\n",
       "      <td>0.997442</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>33.55</td>\n",
       "      <td>-117.70</td>\n",
       "      <td>2.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0549</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.312457</td>\n",
       "      <td>1.085092</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>2.244384</td>\n",
       "      <td>33.93</td>\n",
       "      <td>-116.93</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
       "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
       "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
       "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
       "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.43             1.442  \n",
       "1    -117.39             1.687  \n",
       "2    -122.98             1.621  \n",
       "3    -117.70             2.621  \n",
       "4    -116.93             0.956  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 来测试一下看看能不能正常读取\n",
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3662e524-ced6-4922-b2f4-6b732f903e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
      "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
     ]
    }
   ],
   "source": [
    "with open(train_filepaths[0]) as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3d8b332-48be-41d8-942d-66c4770c96a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets\\\\housing\\\\my_train_00.csv',\n",
       " 'datasets\\\\housing\\\\my_train_01.csv',\n",
       " 'datasets\\\\housing\\\\my_train_02.csv',\n",
       " 'datasets\\\\housing\\\\my_train_03.csv',\n",
       " 'datasets\\\\housing\\\\my_train_04.csv',\n",
       " 'datasets\\\\housing\\\\my_train_05.csv',\n",
       " 'datasets\\\\housing\\\\my_train_06.csv',\n",
       " 'datasets\\\\housing\\\\my_train_07.csv',\n",
       " 'datasets\\\\housing\\\\my_train_08.csv',\n",
       " 'datasets\\\\housing\\\\my_train_09.csv',\n",
       " 'datasets\\\\housing\\\\my_train_10.csv',\n",
       " 'datasets\\\\housing\\\\my_train_11.csv',\n",
       " 'datasets\\\\housing\\\\my_train_12.csv',\n",
       " 'datasets\\\\housing\\\\my_train_13.csv',\n",
       " 'datasets\\\\housing\\\\my_train_14.csv',\n",
       " 'datasets\\\\housing\\\\my_train_15.csv',\n",
       " 'datasets\\\\housing\\\\my_train_16.csv',\n",
       " 'datasets\\\\housing\\\\my_train_17.csv',\n",
       " 'datasets\\\\housing\\\\my_train_18.csv',\n",
       " 'datasets\\\\housing\\\\my_train_19.csv']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_venv]",
   "language": "python",
   "name": "conda-env-tensorflow_venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
