{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485a003c-980d-42d5-b6e3-6bd91afbf36a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "if IS_COLAB or IS_KAGGLE:\n",
    "    %pip install -q -U tfx\n",
    "    print(\"You can safely ignore the package incompatibility errors.\")\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"data\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e42b04-4d4d-4d94-85d2-5aad6438149d",
   "metadata": {},
   "source": [
    "## 什么是数据API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "843e4cd2-3f84-4847-85d1-aafdaea195b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先来创建一个小数据集来做示例\n",
    "X = tf.range(10)\n",
    "# from_tensor_slices可以将X的切片（沿第一维度去切）组合成一个tf.data.Dataset对象\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eace5c3a-de31-41d1-a540-ea3bceb1ae4f",
   "metadata": {},
   "source": [
    "就相当于这么做:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3bdeeb6-e611-45af-8378-fb0468b10826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5395c1d1-979d-43e0-95b4-7f3f17a798fc",
   "metadata": {},
   "source": [
    "### 链式转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b315f-4d7a-43d6-8c88-5d2c5711877c",
   "metadata": {},
   "source": [
    "将数据集放入到Dataset对象之后就可以使用链式表达式来对其进行操作了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96471ad1-943c-4eef-9348-221b2a154bcc",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 比方说将原数据集复制三次，然后每批7个元素分批输出\n",
    "# 这里要注意，使用repeat方法复制并不会真的在内存中复制3次，并且如果不给repeat()方法传参，他就会复制无数遍数据集\n",
    "# 虽然不会真的复制，链式转换返回的数据集是真是存在于内存中的\n",
    "# 对于batch方法，向其中传递的参数是每批次中元素的个数，可看到最后的一批中只有两个元素\n",
    "# 如果在batch()方法中设置rop_remainder = True，那么就可以删除不满足元素数量要求的批\n",
    "dataset = dataset.repeat(3).batch(7, drop_remainder = False)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ca1090-4d6b-48d7-8c96-c24877b76753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用map方法可以操作数据集中所有元素\n",
    "# 比如说要将数据集中的元素都乘2\n",
    "dataset = dataset.map(lambda x: x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6901acf7-3e69-469f-ad67-f7e88b841ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f28f841e-742f-47d9-b3b1-bc563ed8190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用unbatch方法可以将所有批次中的元素都变成一个张量\n",
    "dataset = dataset.unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61e70828-1ce5-407b-b7d9-7da3d10f600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用filter方法可以简单对每个元素进行过滤\n",
    "# 下面语句的含义是：值保留小于10的元素\n",
    "dataset = dataset.filter(lambda x: x < 10)  # keep only items < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1029b575-fb85-4e8b-8cfe-539a41a7da3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e12c3-10bf-4643-9cdf-8121ba769ed9",
   "metadata": {},
   "source": [
    "### 乱序数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf83c5f-7cfe-4bb5-8024-b6d96af1e05d",
   "metadata": {},
   "source": [
    "已知，当训练集中各实例都相互独立且分布均匀的时候，梯度下降最快  \n",
    "所以使用数据API的shuffle()方法来对数据集进行混洗就很有必要了  \n",
    "实现混洗的原理是，在一开始把数据集的前几个元素放进缓冲区中，每次从缓冲区中随机抽出一个实例，并从剩余实例中抽一个实例放进缓冲区中，直到数据集中所有的实例都遍历完成  \n",
    "  \n",
    "这就像是洗牌，左边有一堆牌，最开始只拿上面的三张牌，随机抽出一张牌放到右边，然后再从左边拿一张牌，再从三张牌中随机抽出一张牌放到右边，...直到所有的牌都在右边  \n",
    "这里的三张牌指的就是缓冲区的容量，很明显，缓冲区得大一点效果才能好。但是实践证明并不能太大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "175a34f5-a8b6-49c3-b4ed-935d0cda2cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 3 0 4 2 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 7 1 0 3 2 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 6 9 8 9 7 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 1 4 5 2 8 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([6 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# 使用range方法创建一个Dataset对象，重复3次\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "# 将数据集混洗，缓冲区容量为3，每个批次元素数量为7\n",
    "dataset = dataset.shuffle(buffer_size=3, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c2b63f8-c83d-48d8-bdc0-98f88a5aadef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 3 0 2 6 7 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9 4 0 2 1 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 6 7 8 9 1 2], shape=(7,), dtype=int64)\n",
      "tf.Tensor([0 5 4 6 3 5 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8], shape=(2,), dtype=int64)\n",
      "tf.Tensor([0 1 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([2 8 1 2 0 9 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([6 5 7 4 0 9 2], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 3 5 8 4 7 8], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 6], shape=(2,), dtype=int64)\n",
      "tf.Tensor([2 3 1 0 5 6 8], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 9 0 4 3 4 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 8 7 6 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([2 1 4 5 3 7 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 8], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 如果在原本已经混洗过的数据集上再复制一次，也是一个非常好的点子\n",
    "# 并且此时复制出的不仅仅是元素，还有混洗的操作\n",
    "# 可以看到复制出来的批也是混洗之后的\n",
    "for item in dataset.repeat(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "657387d6-4b3d-4fa1-9f5e-384f37cd35b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 5 6 4 8], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 0 1 1 7 3 2], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 7 8 9 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 4 6 7 3 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 5], shape=(2,), dtype=int64)\n",
      "tf.Tensor([0 2 3 5 6 4 8], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 0 1 1 7 3 2], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 7 8 9 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 4 6 7 3 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 5], shape=(2,), dtype=int64)\n",
      "tf.Tensor([0 2 3 5 6 4 8], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 0 1 1 7 3 2], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 7 8 9 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 4 6 7 3 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 5], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 如果想在复制的同时不打乱原有混洗的顺序，可以在shuffle()方法中设置reshuffle_each_iteration = False\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=3, seed=42, reshuffle_each_iteration = False).batch(7)\n",
    "\n",
    "# 此时再对混洗之后的批数据集进行复制，就会保持原有的混洗顺序进行复制了\n",
    "for item in dataset.repeat(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6acf9-9385-4526-8a5a-d357f68100ec",
   "metadata": {},
   "source": [
    "### 同时对多个CSV文件进行混洗"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4433d21-1d0c-433e-93db-ff482e1e7851",
   "metadata": {},
   "source": [
    "当面临的数据集非常非常大的时候，使用shuffle对数据集进行混洗可能就不太够用了，毕竟缓冲区相对于整个数据集的大小来说可能有点小马拉大车了  \n",
    "但是可以在使用shuffle之前使用别的方法来对原始完整数据集进行一个预混洗，然后再使用shuffle  \n",
    "  \n",
    "  \n",
    "或者将原数据集分成很多个子文件，然后使用shuffle分别对各个子文件进行混洗后再组合到一起，但是这么做有可能会导致同一个文件中的数据仍然十分接近。  \n",
    "为了避免这种情况可以同时随机读取多个文件的数据，并且使用shuffle方法建立一个公用的缓冲区。虽然这听起来很麻烦，其中还得会用到多线程，Data API只需要几行代码就能实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "081f9437-fb03-4db0-9b45-e13d5f4dedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这次拿加利福尼亚的housing数据来做示例\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "\n",
    "# 准备好训练集的数据，等会儿就混洗训练集的数据\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc3b77-f47b-4819-9145-5c9952756f26",
   "metadata": {},
   "source": [
    "#### 将数据集分成多个csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca5182c5-d582-4eca-aea9-7245adb735c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "    # 在文件都保存好之后返回所有文件的地址\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25e81fed-2f32-4608-b7a8-0169829b9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f615fd3-d1a6-4fa3-b181-19a4babd0293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5214</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.049945</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>1.605993</td>\n",
       "      <td>37.63</td>\n",
       "      <td>-122.43</td>\n",
       "      <td>1.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.3275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.490060</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>3.443340</td>\n",
       "      <td>33.69</td>\n",
       "      <td>-117.39</td>\n",
       "      <td>1.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.542373</td>\n",
       "      <td>1.591525</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>2.250847</td>\n",
       "      <td>38.44</td>\n",
       "      <td>-122.98</td>\n",
       "      <td>1.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.1736</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.289003</td>\n",
       "      <td>0.997442</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>33.55</td>\n",
       "      <td>-117.70</td>\n",
       "      <td>2.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0549</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.312457</td>\n",
       "      <td>1.085092</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>2.244384</td>\n",
       "      <td>33.93</td>\n",
       "      <td>-116.93</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
       "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
       "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
       "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
       "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.43             1.442  \n",
       "1    -117.39             1.687  \n",
       "2    -122.98             1.621  \n",
       "3    -117.70             2.621  \n",
       "4    -116.93             0.956  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 来测试一下看看能不能正常读取\n",
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3662e524-ced6-4922-b2f4-6b732f903e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
      "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
     ]
    }
   ],
   "source": [
    "with open(train_filepaths[0]) as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3d8b332-48be-41d8-942d-66c4770c96a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets\\\\housing\\\\my_train_00.csv',\n",
       " 'datasets\\\\housing\\\\my_train_01.csv',\n",
       " 'datasets\\\\housing\\\\my_train_02.csv',\n",
       " 'datasets\\\\housing\\\\my_train_03.csv',\n",
       " 'datasets\\\\housing\\\\my_train_04.csv',\n",
       " 'datasets\\\\housing\\\\my_train_05.csv',\n",
       " 'datasets\\\\housing\\\\my_train_06.csv',\n",
       " 'datasets\\\\housing\\\\my_train_07.csv',\n",
       " 'datasets\\\\housing\\\\my_train_08.csv',\n",
       " 'datasets\\\\housing\\\\my_train_09.csv',\n",
       " 'datasets\\\\housing\\\\my_train_10.csv',\n",
       " 'datasets\\\\housing\\\\my_train_11.csv',\n",
       " 'datasets\\\\housing\\\\my_train_12.csv',\n",
       " 'datasets\\\\housing\\\\my_train_13.csv',\n",
       " 'datasets\\\\housing\\\\my_train_14.csv',\n",
       " 'datasets\\\\housing\\\\my_train_15.csv',\n",
       " 'datasets\\\\housing\\\\my_train_16.csv',\n",
       " 'datasets\\\\housing\\\\my_train_17.csv',\n",
       " 'datasets\\\\housing\\\\my_train_18.csv',\n",
       " 'datasets\\\\housing\\\\my_train_19.csv']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个就是包含所有训练集数据的文件列表\n",
    "train_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44394ed0-cca5-46e1-8188-f8848087dce6",
   "metadata": {},
   "source": [
    "#### 创建一个Input Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "030998db-e1d3-4072-8f82-74b4b9972428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来将使用tf.data.Dataset.list_files函数创建一个文件路径数据集\n",
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27954995-ccd6-486b-bd07-e52faed386f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ShuffleDataset shapes: (), types: tf.string>\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_07.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 文件路径数据集是包含所有文件路径的张量，以二进制的形式保存\n",
    "# 并且文件名是以乱序来保存的，所以可以看到filepath_dataset是ShuffleDataset类型的数据集\n",
    "# 将文件名打乱后保存是一个很好的做法，毕竟数据越乱序越好嘛，但是也可以使用shuffle = False取消乱序\n",
    "print(filepath_dataset)\n",
    "for filepath in filepath_dataset:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43932e5-e991-4dc2-9f32-569f585abc42",
   "metadata": {},
   "source": [
    "接下来就将使用数据集对象的`interleave`方法将各个文件路径对应的数据交织起来  \n",
    "`interleave`方法的第一个参数就是可以映射每个元素（这里的元素时文件地址）的函数（要求放回一个数据子集），然后再使用函数返回的数据子集进行交织  \n",
    "交织的方法就是每次都从多个数据子集中取一行元素，直到数都遍历完了为止。其中`cycle_length`就是设置一次从多少个数据子集中取数  \n",
    "  \n",
    "这里给`interleave`传递的函数含义就是使用`TextLineDataset`类来读取每个文件的数据，然后返回数据子集（去掉第一行）  \n",
    "向`TextLineDataset`类传递一个或多个文本数据的文件地址可以返回一整个数据集，传递的不一定要字符串张量，Python字符串也行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c5934643-b92d-456a-8d3c-9f8a18fa6be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "#tf.data.experimental.AUTOTUNE = 1\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04873524-4528-484e-b497-31744b45e9d7",
   "metadata": {},
   "source": [
    "为了交织的效果能好，各个数据文件的长度最好相等，不然最长文件的结尾将不会被交织  \n",
    "默认情况下数据集对象的`interleave`方法不会使用并行，但是可以通过设置`num_parallel_calls`参数来设置来并行的线程数  \n",
    "再或者设置`tf.data.experimental.AUTOTUNE = 1`，这样TensorFlow就会根据可用的CPU动态选择合适的线程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1b951032-9a5f-4b01-b3a5-95721175da81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n",
      "b'2.4792,24.0,3.4547038327526134,1.1341463414634145,2251.0,3.921602787456446,34.18,-118.38,2.0'\n",
      "b'4.5909,16.0,5.475877192982456,1.0964912280701755,1357.0,2.9758771929824563,33.63,-117.71,2.418'\n",
      "b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782'\n",
      "b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n"
     ]
    }
   ],
   "source": [
    "# 看看数据集前5行\n",
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dade15-797b-4608-9ff9-eec1d7d4819a",
   "metadata": {},
   "source": [
    "在继续进行下去之前，需要首先介绍一下tf中参与数据类型转换的`tf.io.decode_csv`函数，它可以将原本CSV格式的目标序列的每个元素转换成参考序列中各元素的格式，它接受两个非关键字参数是：`records`和`record_defaults`， `records`就是目标序列，`record_defaults`是参考序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cffc7bce-5004-4677-87b2-051b13cf48e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=3.0>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'4'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意第四个元素被转换成了字符串\n",
    "record_defaults=[0, np.nan, tf.constant(np.nan, dtype=tf.float64), \"Hello\", tf.constant([])]\n",
    "parsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)\n",
    "parsed_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7b8ee72c-b105-44e0-86ef-7caeae9a4890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'Hello'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果目标序列的元素有缺失，就会直接使用参考序列相应位置的元素来代替\n",
    "parsed_fields = tf.io.decode_csv(',,,,5', record_defaults)\n",
    "parsed_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8e624d64-aeaa-4a6a-9e65-463fc4900a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "# 但若在某位置上的目标和参考序列都缺失，那就会报错\n",
    "# 就比如下面这个例子，目标序列的5缺失了，并且参考序列对应位置上只有个定义：tf.constant([])，并没有值，\n",
    "# 所以也没办法用参考序列的值进行插补，这时就会这样报错\n",
    "try:\n",
    "    parsed_fields = tf.io.decode_csv('1,2,3,4,', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dac5d565-0dde-4afd-b19a-8b18122c924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect 5 fields but have 7 in record 0 [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "# 并且目标序列和参考序列的长度一定要一致，不然也会报错\n",
    "try:\n",
    "    parsed_fields = tf.io.decode_csv('1,2,3,4,5,6,7', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_venv]",
   "language": "python",
   "name": "conda-env-tensorflow_venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
